<!DOCTYPE html>
<html>

  <head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-47S0JRRR6X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-47S0JRRR6X');
</script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ashudeep  Singh</title>
<meta name="description" content="Principal Applied Scientist, Microsoft AI.
I am a Principal Applied Scientist at Microsoft AI where I build machine learning algorithms for search and recommendation systems at scale.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"  integrity="sha512-Avb2QiuDEEvB4bZJYdft2mNjVShBftLdPG8FJ0V7irTLQ8Uo0qcPxh4Plq7G5tGm0rU+1SPhVotteLpBERwTkw==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:%6D%61%69%6C@%61%73%68%75%64%65%65%70%73%69%6E%67%68.%63%6F%6D"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=zVQOtVEAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/ashudeep" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/AshudeepSingh" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://x.com/AshudeepSingh" target="_blank" title="X"><i class="fab fa-twitter"></i></a>
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/">
                about
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/CV_AshudeepSingh.pdf" target="_blank" rel="noopener">
                cv
                
              </a>
          </li>
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/Resume_AshudeepSingh.pdf" target="_blank" rel="noopener">
                resume
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Ashudeep   Singh
    </h1>
    <h6>Principal Applied Scientist, Microsoft AI. <br> Ph.D., Computer Science, Cornell University. <br> <a href='/assets/pdf/Resume_AshudeepSingh.pdf'>R&eacute;sum&eacute;</a> &middot; <a href='/assets/pdf/CV_AshudeepSingh.pdf'>CV</a></h6>
  </header>
  <br>
  <article>
    
    <div class="profile float-right">
      
        <div class="profile-image-container">
          <img class="img-fluid z-depth-1 rounded profile-image" src="/assets/img/prof_pic.jpg">
          
            <img class="img-fluid z-depth-1 rounded profile-image-hover" src="/assets/img/prof_pic_hover.jpg">
          
        </div>
      
      
    </div>
    

    <div class="clearfix">
      <p><strong>Areas of Interest</strong>: AI Safety &amp; Alignment, Responsible AI, Large Language Models, Machine Learning, Information Retrieval.</p>

<p>I am a Principal Applied Scientist at Microsoft AI, focusing on agentic AI for search systems. My work develops safe, human-aligned multimodal LLMs that learn from feedback while ensuring trustworthy deployment at scale.</p>

<p>Previously, I worked at Pinterest Labs, pioneering bias evaluation frameworks and red-teaming methodologies for LLMs and image generation models in production. I also established Pinterest‚Äôs fairness evaluation and mitigation frameworks across recommendation systems. Prior to Pinterest, I completed my PhD at Cornell University under <a href="http://www.cs.cornell.edu/people/tj/" target="\_blank">Thorsten Joachims</a>, establishing foundational work in fairness for ranking and recommender systems. I also completed research internships at Google Brain, Facebook Research, and Microsoft Research.</p>

<p>My research bridges AI safety theory with practical deployment, publishing at top venues (NeurIPS, ICML, SIGIR, KDD, FAccT) while shipping state-of-the-art responsible AI systems serving hundreds of millions of users. I also serve as Area Chair for NeurIPS and ICML, and co-taught the NeurIPS 2022 tutorial on responsible ML.</p>

<!-- 
  **Areas of Interest**: Machine Learning, Responsible AI, Ranking & Recommender systems, Information Retrieval, AI Safety.
  
  I am an Applied Scientist at Microsoft AI, where I focus on applied research in search, recommendation, and other user interaction systems. My work emphasizes the development of practical multimodal deep neural networks that learn from human interactions and leverage LLMs, VLMs, and Generative AI. I am also passionate about Responsible AI, aiming to ensure fairness and safety for diverse user communities.

  Prior to joining Microsoft, I was an Applied Scientist at Pinterest Labs, specializing in Responsible AI and machine learning fairness. My work at Pinterest focused on developing AI and ML systems that balanced user value with fairness and safety for all stakeholders.
    
  I completed my PhD in Computer Science at Cornell University in 2021 where I was advised by [Thorsten Joachims](http://www.cs.cornell.edu/people/tj/){:target="\_blank"}. My PhD thesis focused on building machine learning models and algorithms to **learn from interactive user feedback** in user-facing platforms such as search and recommendation. I developed a notion of fairness in recommender and ranking systems to ensure fairness to users and creators/producers. 

  During my PhD, I was fortunate to have [Solon Barocas](http://solon.barocas.org/), [Karthik Sridharan](https://www.cs.cornell.edu/~sridharan/){:target="\_blank"}, and [David Mimno](https://mimno.infosci.cornell.edu/){:target="\_blank"} on my dissertation committee. I also completed internships at Google Brain, Facebook Research, and Microsoft Research (NYC and Montreal), where I had the opportunity to collaborate closely with [Alex Beutel](http://alexbeutel.com/), [Fernando Diaz](https://841.io/), [Khalid El-Arini](http://www.khalidelarini.com/), [John Langford](https://www.microsoft.com/en-us/research/people/jcl/). Previously, I was an undergraduate student at the Indian Institute of Technology (IIT) Kanpur and also spent a summer doing NLP research at Carnegie Mellon University. 
  
  You can find more information on my [resume](/assets/pdf/Resume_AshudeepSingh.pdf) and [CV](/assets/pdf/CV_AshudeepSingh.pdf). -->

<hr />


    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>May 17, 2024</abbr>
          </th>
          <td>
            
              Paper on measuring user impact of diversification in recommendations titled ‚ÄúInclusive Recommendations and User Engagement: Experimental Evidence from Pinterest‚Äù accepted at <a href="https://ec24.sigecom.org/program/accepted-papers/">ACM EC 2024</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Oct 17, 2023</abbr>
          </th>
          <td>
            
              Presented a guest lecture for the Operations Management class (BUAD 311) at USC Marshall School of Business on ‚ÄúResponsible ML for Real-World Search and Recommender Systems: A Multistakeholder Perspective‚Äù <a href="assets/pdf/usc-talk.pdf">(slides)</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Sep 9, 2023</abbr>
          </th>
          <td>
            
              Ongoing research with <a href="https://madhavkumar.com/">Madhav Kumar</a>(MIT) on the impact of diversification in recommender systems on its users was recently accepted to <a href="https://sites.google.com/view/stagewise2023/home">Workshop on Information Systems and Economics (WISE) 2023</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Jun 15, 2023</abbr>
          </th>
          <td>
            
              Our work on interpretability for recommender systems called ‚ÄúRecRec: Algorithmic Recourse for Recommender Systems‚Äù was accepted to <a href="https://uobevents.eventsair.com/cikm2023/">ACM CIKM 2023</a>. This is joint work with co-authors from University of Washington and University of Maryland. Here is the <a href="https://arxiv.org/abs/2308.14916">arXiv link</a> for the paper.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Jun 11, 2023</abbr>
          </th>
          <td>
            
              Our recent work on <a href="https://dl.acm.org/doi/10.1145/3593013.3594112">diversification in search and recommender systems</a> across the Pinterest platform was accepted to <a href="https://facctconference.org/2023/">ACM FAccT 2023</a>. See you in Chicago! <br />

It was recently also published on the <a href="https://medium.com/pinterest-engineering/representation-online-matters-practical-end-to-end-diversification-in-search-and-recommender-cb60b547f2e0">Pinterest Engineering blog</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Dec 5, 2022</abbr>
          </th>
          <td>
            
              The slides to the NeurIPS 2022 tutorial on Fair and Socially Responsible ML for Recommendations are public. Download them <a href="https://fair-recs-tutorial.github.io/neurips-2022-tutorial/">here</a>. Visit the <a href="https://neurips.cc/virtual/2022/tutorial/55805">NeurIPS portal</a> to access the recording video if you are registered.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Sep 28, 2022</abbr>
          </th>
          <td>
            
              <a href="https://www.linkedin.com/in/hannah-korevaar-5163b463/">Hannah Korevaar</a> (Meta), <a href="https://mraghavan.github.io/">Manish Raghavan</a> (MIT), and I are presenting a tutorial at NeurIPS 2022 on Fair and Socially Responsible ML for Recommendations.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">
            <abbr class="badge" style='background-color: var(--global-theme-color);'>Sep 29, 2021</abbr>
          </th>
          <td>
            
              Our paper <a href="https://arxiv.org/abs/2107.06720">‚ÄúFairness in Ranking under Uncertainty‚Äù</a> has been accepted to NeurIPS 2021. This is joint work with my advisor Thorsten Joachims (Cornell) and David Kempe from USC. üìÑ

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>
<hr>
    


    
      <div class="publications">
  <h2>selected publications</h2>
  Find more information on the <a href='publications'>publications</a> page and <a href='https://scholar.google.com/citations?user=zVQOtVEAAAAJ&hl=en'>google scholar</a>.
  <br>
  <p>* contributed equally.</p>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">KDD</abbr>
    
    
    
  </div>

  <div id="singh2018fairness" class="col-sm-8">
    
    <div class="title">Fairness of Exposure in Rankings</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, </em>
      
      
      2018. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/1802.07281" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Rankings are ubiquitous in the online world today. As we have transitioned from finding books in libraries to ranking products, jobs, job applicants, opinions and potential romantic partners, there is a substantial precedent that ranking systems have a responsibility not only to their users but also to the items being ranked. To address these often conflicting responsibilities, we propose a conceptual and computational framework that allows the formulation of fairness constraints on rankings in terms of exposure allocation. As part of this framework, we develop efficient algorithms for finding rankings that maximize the utility for the user while provably satisfying a specifiable notion of fairness. Since fairness goals can be application specific, we show how a broad range of fairness constraints can be implemented using our framework, including forms of demographic parity, disparate treatment, and disparate impact constraints. We illustrate the effect of these constraints by providing empirical results on two ranking problems.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">NeurIPS</abbr>
    
    
    
  </div>

  <div id="singh2019policy" class="col-sm-8">
    
    <div class="title">Policy Learning for Fairness in Ranking</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Neural Information Processing Systems (NeurIPS), </em>
      
      
      2019. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/1902.04056" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      
      
      
      
      <a href="https://github.com/ashudeep/Fair-PGRank" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="http://papers.nips.cc/paper/8782-policy-learning-for-fairness-in-ranking" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Conventional Learning-to-Rank (LTR) methods optimize the utility of the rankings to the users, but they are oblivious to their impact on the ranked items. However, there has been a growing understanding that the latter is important to consider for a wide range of ranking applications (e.g. online marketplaces, job placement, admissions). To address this need, we propose a general LTR framework that can optimize a wide range of utility metrics (e.g. NDCG) while satisfying fairness of exposure constraints with respect to the items. This framework expands the class of learnable ranking functions to stochastic ranking policies, which provides a language for rigorously expressing fairness specifications. Furthermore, we provide a new LTR algorithm called Fair-PG-Rank for directly searching the space of fair ranking policies via a policy-gradient approach. Beyond the theoretical evidence in deriving the framework and the algorithm, we provide empirical results on simulated and real-world datasets verifying the effectiveness of the approach in individual and group-fairness settings.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">SIGIR</abbr>
    
    
        <abbr class="badge" style='background-color: var(--global-tag-color);'>Best Paper Award</abbr>
      
    
  </div>

  <div id="morik2020controlling" class="col-sm-8">
    
    <div class="title">Controlling Fairness and Bias in Dynamic Learning-to-Rank</div>

    <div class="author">
      
      
      
      
      
      Marco Morik*,
      
      
      
      
      
      
      
      
      
      Ashudeep Singh*,
      
      
      
      
      
      
      
      
      
      Jessica Hong,
      
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval, </em>
      
      
      2020. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/2005.14713" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      
      
      
      
      <a href="https://github.com/MarcoMorik/Dynamic-Fairness" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="/assets/pdf/SIGIR-Morik-Singh-slides.pdf" class="btn btn-sm z-depth-0" role="button"
        target="_blank">Slides</a>
      
      
      
      <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401100" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Rankings are the primary interface through which many online platforms match users to items (e.g. news, products, music, video). In these two-sided markets, not only the users draw utility from the rankings, but the rankings also determine the utility (e.g. exposure, revenue) for the item providers (e.g. publishers, sellers, artists, studios). It has already been noted that myopically optimizing utility to the users ‚Äì as done by virtually all learning-to-rank algorithms ‚Äì can be unfair to the item providers. We, therefore, present a learning-to-rank approach for explicitly enforcing merit-based fairness guarantees to groups of items (e.g. articles by the same publisher, tracks by the same artist). In particular, we propose a learning algorithm that ensures notions of amortized group fairness, while simultaneously learning the ranking function from implicit feedback data. The algorithm takes the form of a controller that integrates unbiased estimators for both fairness and utility, dynamically adapting both as more data becomes available. In addition to its rigorous theoretical foundation and convergence guarantees, we find empirically that the algorithm is highly practical and robust.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">FAccTRec</abbr>
    
    
    
  </div>

  <div id="singh2020building" class="col-sm-8">
    
    <div class="title">Building Healthy Recommendation Sequences for Everyone: A Safe Reinforcement Learning Approach</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      Yoni Halpern,
      
      
      
      
      
      
      
      
      
      Nithum Thain,
      
      
      
      
      
      
      
      
      
      Konstantina Christakopoulou,
      
      
      
      
      
      
      
      
      
      <a href="https://sites.google.com/view/edchi/" target="_blank">Ed H. Chi</a>,
      
      
      
      
      
      
      
      
      
      Jilin Chen,
      
      
      
      
      
      
      
      
      
      and <a href="http://www.alexbeutel.com" target="_blank">Alex Beutel</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In FAccTRec Workshop at ACM RecSyS, </em>
      
      
      2020. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="http://www.ashudeepsingh.com/publications/facctrec2020_singh_et_al.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      <a href="https://github.com/google/ml-fairness-gym/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="/assets/pdf/facctrec2020_singh_et_al_slides.pdf" class="btn btn-sm z-depth-0" role="button"
        target="_blank">Slides</a>
      
      
      
      <a href="/safe-rl-recs/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A key consideration in the design of recommender systems is the long term well-being of users. In this work, we formalize this challenge as a multi-objective, safe reinforcement learning problem, balancing positive user feedback and the ‚Äúhealthiness‚Äù of user trajectories. We note that in some cases, naively balancing these objectives can lead to unhealthy experiences, even if unlikely, still occurring in a small subset of users leading us to examine a distributional notion of recommendation safety. Thus, we propose a reinforcement learning approach that optimizes for positive feedback from users while simultaneously optimizing for the health of worst-case users to remain high. To empirically validate our method, we develop a novel research simulation environment motivated by a movie recommendation setting that considers exposure to violence as a proxy for unhealthy recommendations. We demonstrate how our method reduces unhealthy recommendations to the most vulnerable users, without sacrificing much user satisfaction.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">FAccT</abbr>
    
    
    
  </div>

  <div id="silva2023representation" class="col-sm-8">
    
    <div class="title">Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems</div>

    <div class="author">
      
      
      
      
      
      Pedro Silva,
      
      
      
      
      
      
      
      
      
      Bhawna Juneja,
      
      
      
      
      
      
      
      
      
      Shloka Desai,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="https://www.linkedin.com/in/nadiafawaz/" target="_blank">Nadia Fawaz</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, </em>
      
      
      2023. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/2305.15534" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-OR logical operator to bucketized retrieval at the retrieval stage and from greedy re-rankers to multi-objective optimization using determinantal point processes for the ranking stage, balances diversity and utility while enabling fast iterations and scalable expansion to diversification over multiple dimensions. Our experiments indicate that these approaches significantly improve diversity metrics, with a neutral to a positive impact on utility metrics and improved user satisfaction, both qualitatively and quantitatively, in production.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">FnTIR¬Æ Journal</abbr>
    
    
    
  </div>

  <div id="INR-101" class="col-sm-8">
    
    <div class="title">Fairness in Search Systems</div>

    <div class="author">
      
      
      
      
      
      <a href="https://www.cse.scu.edu/~yfang/" target="_blank">Yi Fang</a>,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and Zhiqiang Tao
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>Foundations and Trends¬Æ in Information Retrieval</em>
      
      
      2024. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Search engines play a crucial role in organizing and delivering information to billions of users worldwide. However, these systems often reflect and amplify existing societal biases and stereotypes through their search results and rankings. This concern has prompted researchers to investigate methods for measuring and reducing algorithmic bias, with the goal of developing more equitable search systems. This monograph presents a comprehensive taxonomy of fairness in search systems and surveys the current research landscape. We systematically examine how bias manifests across key search components, including query interpretation and processing, document representation and indexing, result ranking algorithms, and system evaluation metrics. By critically analyzing the existing literature, we identify persistent challenges and promising research directions in the pursuit of fairer search systems. Our aim is to provide a foundation for future work in this rapidly evolving field while highlighting opportunities to create more inclusive and equitable information retrieval technologies.</p>
    </div>
    
  </div>
</div></li></ol>
</div>

    

    <!--  -->

    
    <div class="social">
      <span class="contact-icon text-center">
  <a href="mailto:%6D%61%69%6C@%61%73%68%75%64%65%65%70%73%69%6E%67%68.%63%6F%6D"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=zVQOtVEAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/ashudeep" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/AshudeepSingh" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://x.com/AshudeepSingh" target="_blank" title="X"><i class="fab fa-twitter"></i></a>
  
  
  
  
</span>

      <div class="contact-note">Email me at <a href="mailto:mail@ashudeepsingh.com">mail@ashudeepsingh.com</a> or reach out via X or Linkedin.
</div>
    </div>
    

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Ashudeep  Singh.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    Last updated: November 03, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-56982603-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-56982603-1');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
