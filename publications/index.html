<!DOCTYPE html>
<html>

  <head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-47S0JRRR6X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-47S0JRRR6X');
</script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ashudeep  Singh | publications</title>
<meta name="description" content="Principal Applied Scientist, Microsoft AI.
I am a Principal Applied Scientist at Microsoft AI where I build machine learning algorithms for search and recommendation systems at scale.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"  integrity="sha512-Avb2QiuDEEvB4bZJYdft2mNjVShBftLdPG8FJ0V7irTLQ8Uo0qcPxh4Plq7G5tGm0rU+1SPhVotteLpBERwTkw==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://www.ashudeepsingh.com/">
       <span class="font-weight-bold">Ashudeep</span>   Singh
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/resume/">
                resume
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">Find more information on <a href='https://scholar.google.com/citations?user=zVQOtVEAAAAJ&hl=en'>Google scholar</a>. </br> * contributed equally.</p>
  </header>

  <article>
    <div class="publications">


  <!-- <h2 class="year">2025</h2> -->
  <ol class="bibliography"></ol>

  <!-- <h2 class="year">2024</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">ACM EC</abbr>
    
    
    
  </div>

  <div id="kumar2024inclusive" class="col-sm-8">
    
    <div class="title">Inclusive Recommendations and User Engagement: Experimental Evidence from Pinterest</div>

    <div class="author">
      
      
      
      
      
      <a href="https://madhavkumar.com/" target="_blank">Madhav Kumar</a>,
      
      
      
      
      
      
      
      
      
      Pedro Silva,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and Abhay Varmaraja
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In ACM International Conference on Economics and Computation (EC’24), </em>
      
      
      2024. 
      
      
      <br /><em>Also appeared at WISE 2023 and CIST 2024. Won the Best Paper Award at CIST 2024.</em>
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="/assets/pdf/inclusive_recommendations.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the impact of diversifying recommendations for inclusivity on one of the largest visual content discovery platforms in the world, Pinterest. Pinterest re-designed its recommendation systems to improve the representation of all skin tones in recommended content and foster a more inclusive user experience. We describe the design of the new recommendation system and present results from a field experiment in which users across six countries were randomly assigned to receive a more diverse set of recommendations based on content skin tone. We find that the overall engagement rates remain stable and engagement with previously underrepresented content increases significantly. More broadly, users diversify their consumption by engaging with content from all skin tone ranges. We shed light on the mechanism driving these results using heterogeneous treatment effect analysis. We find that engagement for users with preference for deeper skin tone content increases significantly and engagement for users with preference for lighter skin tone content remains relatively stable. Finally, we analyze post-launch data to better understand the long-term implications of diversifying recommendations. Our research provides practical insights for platform managers and policymakers to create inclusive digital environments that promote engagement while catering to diverse user preferences. </p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2023</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">FAccT</abbr>
    
    
    
  </div>

  <div id="silva2023representation" class="col-sm-8">
    
    <div class="title">Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems</div>

    <div class="author">
      
      
      
      
      
      Pedro Silva,
      
      
      
      
      
      
      
      
      
      Bhawna Juneja,
      
      
      
      
      
      
      
      
      
      Shloka Desai,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="https://www.linkedin.com/in/nadiafawaz/" target="_blank">Nadia Fawaz</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, </em>
      
      
      2023. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/2305.15534" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-OR logical operator to bucketized retrieval at the retrieval stage and from greedy re-rankers to multi-objective optimization using determinantal point processes for the ranking stage, balances diversity and utility while enabling fast iterations and scalable expansion to diversification over multiple dimensions. Our experiments indicate that these approaches significantly improve diversity metrics, with a neutral to a positive impact on utility metrics and improved user satisfaction, both qualitatively and quantitatively, in production.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">CIKM</abbr>
    
    
    
  </div>

  <div id="verma2023recrec" class="col-sm-8">
    
    <div class="title">RecRec: Algorithmic Recourse for Recommender Systems</div>

    <div class="author">
      
      
      
      
      
      <a href="https://vsahil.github.io/" target="_blank">Sahil Verma</a>,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      Varich Boonsanong,
      
      
      
      
      
      
      
      
      
      <a href="https://jpdickerson.com/" target="_blank">John P. Dickerson</a>,
      
      
      
      
      
      
      
      
      
      and <a href="https://ischool.uw.edu/people/faculty/profile/chirags" target="_blank">Chirag Shah</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, </em>
      
      
      2023. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/2308.14916" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recommender systems play an essential role in the choices people make in domains such as entertainment, shopping, food, news, employment, and education. 
The machine learning models underlying these recommender systems are often enormously large and black-box in nature for users, content providers, and system developers alike. 
It is often crucial for all the stakeholders to understand the model’s rationale behind making certain predictions and recommendations. 
This is especially true for the content providers whose livelihoods depend on the recommender system. 
Drawing motivation from the practitioners’ need, in this work, we propose a recourse framework for recommender systems, targeted towards the content providers. 
Algorithmic recourse in the recommendation setting is a set of actions, which, if executed, would modify the recommendations (or ranking) of an item in the desired manner. 
A recourse suggests actions of the form: "if a feature changes X to Y, then the ranking of that item for a set of users will change to Z." 
Furthermore, we demonstrate that RecRec is highly effective in generating valid, sparse, and actionable recourses through an empirical evaluation of recommender systems trained on three real world datasets. 
To the best of our knowledge, this work is the first to conceptualize and empirically test a generalized framework for generating recourses for recommender systems. </p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2022</h2> -->
  <ol class="bibliography"></ol>

  <!-- <h2 class="year">2021</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">NeurIPS</abbr>
    
    
    
  </div>

  <div id="singh2021uncertainty" class="col-sm-8">
    
    <div class="title">Fairness in Ranking under Uncertainty</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      <a href="https://david-kempe.com/" target="_blank">David Kempe</a>,
      
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Neural Information Processing Systems (NeurIPS), </em>
      
      
      2021. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/2107.06720" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      <a href="https://github.com/ashudeep/Uncertainity-based-fairness-for-rankings" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="https://proceedings.neurips.cc/paper/2021/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Fairness has emerged as an important consideration in algorithmic decision-making. Unfairness occurs when an agent with higher merit obtains a worse outcome than an agent with lower merit. Our central point is that a primary cause of unfairness is uncertainty. A principal or algorithm making decisions never has access to the agents’ true merit, and instead uses proxy features that only imperfectly predict merit (e.g., GPA, star ratings, recommendation letters). None of these ever fully capture an agent’s merit; yet existing approaches have mostly been defining fairness notions directly based on observed features and outcomes. Our primary point is that it is more principled to acknowledge and model the uncertainty explicitly. The role of observed features is to give rise to a posterior distribution of the agents’ merits. We use this viewpoint to define a notion of approximate fairness in ranking. We call an algorithm ϕ-fair (for ϕ∈[0,1]) if it has the following property for all agents x and all k: if agent x is among the top k agents with respect to merit with probability at least ρ (according to the posterior merit distribution), then the algorithm places the agent among the top k agents in its ranking with probability at least ϕρ. We show how to compute rankings that optimally trade off approximate fairness against utility to the principal. In addition to the theoretical characterization, we present an empirical analysis of the potential impact of the approach in simulation studies. For real-world validation, we applied the approach in the context of a paper recommendation system that we built and fielded at the KDD 2020 conference.
  </p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2020</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">SIGIR</abbr>
    
    
        <abbr class="badge" style="background-color: var(--global-tag-color);">Best Paper Award</abbr>
      
    
  </div>

  <div id="morik2020controlling" class="col-sm-8">
    
    <div class="title">Controlling Fairness and Bias in Dynamic Learning-to-Rank</div>

    <div class="author">
      
      
      
      
      
      Marco Morik*,
      
      
      
      
      
      
      
      
      
      Ashudeep Singh*,
      
      
      
      
      
      
      
      
      
      Jessica Hong,
      
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval, </em>
      
      
      2020. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/2005.14713" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      <a href="https://github.com/MarcoMorik/Dynamic-Fairness" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="/assets/pdf/SIGIR-Morik-Singh-slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
      
      
      <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401100" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Rankings are the primary interface through which many online platforms match users to items (e.g. news, products, music, video). In these two-sided markets, not only the users draw utility from the rankings, but the rankings also determine the utility (e.g. exposure, revenue) for the item providers (e.g. publishers, sellers, artists, studios). It has already been noted that myopically optimizing utility to the users – as done by virtually all learning-to-rank algorithms – can be unfair to the item providers. We, therefore, present a learning-to-rank approach for explicitly enforcing merit-based fairness guarantees to groups of items (e.g. articles by the same publisher, tracks by the same artist). In particular, we propose a learning algorithm that ensures notions of amortized group fairness, while simultaneously learning the ranking function from implicit feedback data. The algorithm takes the form of a controller that integrates unbiased estimators for both fairness and utility, dynamically adapting both as more data becomes available. In addition to its rigorous theoretical foundation and convergence guarantees, we find empirically that the algorithm is highly practical and robust.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">FAccTRec</abbr>
    
    
    
  </div>

  <div id="singh2020building" class="col-sm-8">
    
    <div class="title">Building Healthy Recommendation Sequences for Everyone: A Safe Reinforcement Learning Approach</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      Yoni Halpern,
      
      
      
      
      
      
      
      
      
      Nithum Thain,
      
      
      
      
      
      
      
      
      
      Konstantina Christakopoulou,
      
      
      
      
      
      
      
      
      
      <a href="https://sites.google.com/view/edchi/" target="_blank">Ed H. Chi</a>,
      
      
      
      
      
      
      
      
      
      Jilin Chen,
      
      
      
      
      
      
      
      
      
      and <a href="http://www.alexbeutel.com" target="_blank">Alex Beutel</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In FAccTRec Workshop at ACM RecSyS, </em>
      
      
      2020. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="http://www.ashudeepsingh.com/publications/facctrec2020_singh_et_al.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      <a href="https://github.com/google/ml-fairness-gym/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="/assets/pdf/facctrec2020_singh_et_al_slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
      
      
      <a href="/safe-rl-recs/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A key consideration in the design of recommender systems is the long term well-being of users. In this work, we formalize this challenge as a multi-objective, safe reinforcement learning problem, balancing positive user feedback and the “healthiness” of user trajectories. We note that in some cases, naively balancing these objectives can lead to unhealthy experiences, even if unlikely, still occurring in a small subset of users leading us to examine a distributional notion of recommendation safety. Thus, we propose a reinforcement learning approach that optimizes for positive feedback from users while simultaneously optimizing for the health of worst-case users to remain high. To empirically validate our method, we develop a novel research simulation environment motivated by a movie recommendation setting that considers exposure to violence as a proxy for unhealthy recommendations. We demonstrate how our method reduces unhealthy recommendations to the most vulnerable users, without sacrificing much user satisfaction.</p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2019</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">NeurIPS</abbr>
    
    
    
  </div>

  <div id="singh2019policy" class="col-sm-8">
    
    <div class="title">Policy Learning for Fairness in Ranking</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Neural Information Processing Systems (NeurIPS), </em>
      
      
      2019. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/1902.04056" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      <a href="https://github.com/ashudeep/Fair-PGRank" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
      <a href="http://papers.nips.cc/paper/8782-policy-learning-for-fairness-in-ranking" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Conventional Learning-to-Rank (LTR) methods optimize the utility of the rankings to the users, but they are oblivious to their impact on the ranked items. However, there has been a growing understanding that the latter is important to consider for a wide range of ranking applications (e.g. online marketplaces, job placement, admissions). To address this need, we propose a general LTR framework that can optimize a wide range of utility metrics (e.g. NDCG) while satisfying fairness of exposure constraints with respect to the items. This framework expands the class of learnable ranking functions to stochastic ranking policies, which provides a language for rigorously expressing fairness specifications. Furthermore, we provide a new LTR algorithm called Fair-PG-Rank for directly searching the space of fair ranking policies via a policy-gradient approach. Beyond the theoretical evidence in deriving the framework and the algorithm, we provide empirical results on simulated and real-world datasets verifying the effectiveness of the approach in individual and group-fairness settings.</p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2018</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">KDD</abbr>
    
    
    
  </div>

  <div id="singh2018fairness" class="col-sm-8">
    
    <div class="title">Fairness of Exposure in Rankings</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, London, UK, </em>
      
      
      2018. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/1802.07281" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Rankings are ubiquitous in the online world today. As we have transitioned from finding books in libraries to ranking products, jobs, job applicants, opinions and potential romantic partners, there is a substantial precedent that ranking systems have a responsibility not only to their users but also to the items being ranked. To address these often conflicting responsibilities, we propose a conceptual and computational framework that allows the formulation of fairness constraints on rankings in terms of exposure allocation. As part of this framework, we develop efficient algorithms for finding rankings that maximize the utility for the user while provably satisfying a specifiable notion of fairness. Since fairness goals can be application specific, we show how a broad range of fairness constraints can be implemented using our framework, including forms of demographic parity, disparate treatment, and disparate impact constraints. We illustrate the effect of these constraints by providing empirical results on two ranking problems.</p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2017</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">NeurIPS Workshop</abbr>
    
    
    
  </div>

  <div id="singh2017equality" class="col-sm-8">
    
    <div class="title">Equality of Opportunity in Rankings</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In Workshop On Prioritizing Online Content (WPOC) at NeurIPS, </em>
      
      
      2017. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="https://www.k4all.org/wp-content/uploads/2017/09/WPOC2017_paper_9.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>With the ubiquity of algorithmic methods for ranking in systems like search engines, recommendations, and advertisements, it becomes essential to ensure that these systems do not impact different groups at different rates. In this work, we focus on defining the notions of fairness in such ranking scenarios in terms of the amount of exposure (opportunity) each item gets in its position while keeping in mind the protected groups. We formulate these definitions for protected groups of documents, queries as well as users.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">NeurIPS Workshop</abbr>
    
    
    
  </div>

  <div id="singh2017learning" class="col-sm-8">
    
    <div class="title">Learning item embeddings using biased feedback</div>

    <div class="author">
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In NeurIPS Workshop on Causal Inference and Machine Learning for Intelligent Decision Making, </em>
      
      
      2017. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="/assets/pdf/unbiased-embeddings.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Learning item embeddings from browsing logs of recommender systems provides intriguing opportunities for understanding user preferences. However, such log data can be severely biased because recommendations imply a selection bias on the number of clicks an item receives. This selection bias can lead to learned embeddings that are distorted by past recommendations and that do not reflect the true semantic similarity one would like to capture. To overcome this problem, we formulate the task of learning embeddings as a counterfactual learning problem: how would the user have clicked, if the recommendation algorithm had not interfered? To demonstrate effectiveness and promise of this approach, we present synthetic experiments that illustrate how the counterfactual learning approach can recover the true embeddings despite biased data.</p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2016</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">ICML</abbr>
    
    
    
  </div>

  <div id="schnabel2016recommendations" class="col-sm-8">
    
    <div class="title">Recommendations as treatments: Debiasing learning and evaluation</div>

    <div class="author">
      
      
      
      
      
      <a href="https://www.microsoft.com/en-us/research/people/toschnab/" target="_blank">Tobias Schnabel</a>,
      
      
      
      
      
      
      
      
      
      <a href="https://www.microsoft.com/en-us/research/people/adswamin/" target="_blank">Adith Swaminathan</a>,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      Navin Chandak,
      
      
      
      
      
      
      
      
      
      and <a href="http://www.cs.cornell.edu/people/tj/" target="_blank">Thorsten Joachims</a>
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In International Conference on Machine Learning (ICML), </em>
      
      
      2016. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/1602.05352" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
      <a href="http://proceedings.mlr.press/v48/schnabel16.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handle selection biases by adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, and find that it is highly practical and scalable.</p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2015</h2> -->
  <ol class="bibliography"></ol>

  <!-- <h2 class="year">2014</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">Preprint</abbr>
    
    
    
  </div>

  <div id="bhartiya2014semantic" class="col-sm-8">
    
    <div class="title">A Semantic Approach to Summarization</div>

    <div class="author">
      
      
      
      
      
      Divyanshu Bhartiya*,
      
      
      
      
      
      
      
      
      
      and Ashudeep Singh*
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>arXiv preprint arXiv:1406.1203</em>
      
      
      2014. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      <a href="http://arxiv.org/abs/1406.1203" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
      
      
      
      
      <a href="/assets/pdf/summarization.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Sentence extraction based summarization has some limitations as it doesn’t go into the semantics of the document. Also, it lacks the capability of sentence generation which is intuitive to humans. Here we present a novel method to summarize text documents taking the process to semantic levels with the use of WordNet and other resources, and using a technique for sentence generation. We involve semantic role labeling to get the semantic representation of text and use of segmentation to form clusters of the related pieces of text. Picking out the centroids and sentence generation completes the task. We evaluate our system against human composed summaries and also present an evaluation done by humans to measure the quality attributes of our summaries.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">ITS</abbr>
    
    
    
  </div>

  <div id="adamson2014predicting" class="col-sm-8">
    
    <div class="title">Predicting Student Learning from Conversational Cues</div>

    <div class="author">
      
      
      
      
      
      David Adamson,
      
      
      
      
      
      
      
      
      
      Akash Bharadwaj,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      Colin Ashe,
      
      
      
      
      
      
      
      
      
      David Yaron,
      
      
      
      
      
      
      
      
      
      and Carolyn P Rosé
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In International Conference on Intelligent Tutoring Systems (ITS), </em>
      
      
      2014. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="/assets/pdf/its2014.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the work here presented, we apply textual and sequential methods to assess the outcomes of an unconstrained multiparty dialogue. In the context of chat transcripts from a collaborative learning scenario, we demonstrate that while low-level textual features can indeed predict student success, models derived from sequential discourse act labels are also predictive, both on their own and as a supplement to textual feature sets. Further, we find that evidence from the initial stages of a collaborative activity is just as effective as using the whole.</p>
    </div>
    
  </div>
</div></li></ol>

  <!-- <h2 class="year">2013</h2> -->
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">AIED</abbr>
    
    
    
  </div>

  <div id="adamson2013automatically" class="col-sm-8">
    
    <div class="title">Automatically Generating Discussion Questions</div>

    <div class="author">
      
      
      
      
      
      David Adamson,
      
      
      
      
      
      
      
      
      
      Divyanshu Bhartiya,
      
      
      
      
      
      
      
      
      
      Biman Gujral,
      
      
      
      
      
      
      
      
      
      Radhika Kedia,
      
      
      
      
      
      
      
      
      <em>Ashudeep Singh</em>,
      
      
      
      
      
      
      
      
      and Carolyn P. Rosé
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>In International Conference on Artificial Intelligence in Education (AIED), </em>
      
      
      2013. 
      
      
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
      
      
      <a href="/assets/pdf/aied2013.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p> Automatic question generation can support instruction and learning. However, work to date has produced mostly “shallow” questions that fall short of supporting deep learning and discussion. We propose an extension to a state-of-the-art question generation system that allows it to produce deep, subjective questions suitable for group discussion. We evaluate the questions generated by this system against a panel of experienced judges, and find that our approach fares significantly better than the baseline system.</p>
    </div>
    
  </div>
</div></li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Ashudeep  Singh.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    Last updated: August 01, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-56982603-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-56982603-1');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
